{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4371d3a",
   "metadata": {},
   "source": [
    "# Lab 4 - Classification with NN\n",
    "\n",
    "Debe formar grupo de hasta 3 personas para los laboratorios.\n",
    "\n",
    "__Estudiante 1:__ Nombre del estudiante 1 \\\n",
    "__Estudiante 2:__ Nombre del estudiante 2 \\\n",
    "__Estudiante 3:__ Nombre del estudiante 3 \n",
    "\n",
    "__Fecha de inicio:__ 10 de Mayo del 2023 \\\n",
    "__Fecha de entrega:__  18 de Mayo del 2023 (11:59 pm)\n",
    "\n",
    "## Introduction\n",
    "Has sido contratado en una empresa , superVegs,  es una empresa de cadena de suministro de productos frescos. Son pioneros en resolver uno de los problemas de la cadena de suministro más difíciles del mundo al aprovechar la tecnología innovadora. Un componente integral de su proceso de automatización es el desarrollo de clasificadores robustos que pueden distinguir entre imágenes de diferentes tipos de vegetales, mientras que también etiquetan correctamente las imágenes que no contienen ningún tipo de vegetal.\n",
    "\n",
    "Como nuevo integrante de la empresa se nos ha encomendado la elaboración de un clasificador multiclase para la identificación de distintos tipos de vegetales. Para ello se estara usando el siguiente dataset [Kaggle Vagetable Image Dataset](https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset)\n",
    "\n",
    "## Instructions \n",
    "Bienvenido a tu primera semana de trabajo en la cual debes crear una aplicaciones de visión artificial! En particular, un clasificador!! \n",
    "\n",
    "Específicamente, se quiere:\n",
    "\n",
    "- Analizar y preprocesar el dataset\n",
    "- Implementar una serie de models clasificadores \n",
    "- Comparar una serie de modelo clasificadores de imágenes en nuevos datos\n",
    "- Medir el rendimiento de los mejores modelos de clasificación en datos de prueba y en el mundo real.\n",
    "- (Optional) Implementar una aplicación [Gradio](https://gradio.app/quickstart/) que se pueda ejecutar en su teléfono o laptop\n",
    "\n",
    "\n",
    "## Requerimientos \n",
    "Kaggle es uno de los mejores lugares para practicar, mejorar y mostrar su habilidad de ML.\n",
    "\n",
    "Lo más importante es que obtendremos **GPU gratis 30 horas por semana.**\n",
    "\n",
    "Por lo tanto, se recomienda usar Kaggle Kernels para la mayoría de sus experimentos.\n",
    "\n",
    "\n",
    "Su Notebook debe tener lo siguiente:\n",
    "\n",
    "- Analizar los datos\n",
    "- Cargar los datos\n",
    "- Configurar varios modelos de NN\n",
    "- Métricas de evaluación\n",
    "- Entrenamiento y Validación\n",
    "- Experimentos en imagenes Reales\n",
    "- Enlace de perfil de Kaggle o a la notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c540a4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e30dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb1883",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis¶\n",
    "\n",
    "La mayoría de los datasets se dividen en diferentes divisiones. Por ejemplo, a menudo verá un subconjunto de datos de entrenamiento, que se usa para construir el modelo, un subconjunto de datos de validación, que se usa para medir el rendimiento del modelo mientras se entrena, y un conjunto de datos de prueba que se usa para medir el rendimiento del modelo al final del entrenamiento, y generalmente se considera qué tan bien funcionará el modelo en el mundo real.\n",
    "\n",
    "Específicamente, usaremos el conjunto de datos de vegetales que está disponible de forma gratuita en Kaggle: https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset\n",
    "\n",
    "### TODO \n",
    "- Cargar las imagenes \n",
    "- Revisar el tipo de las imagenes y sus caracteristicas\n",
    "- Revisar al menos una imagen por clase\n",
    "- Analizar la distribucion de imagenes por clases \n",
    "- Cualquier otro analisis que desee realizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051d478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './vegetable'\n",
    "\n",
    "# TODO: Debe cargar las imagenes y ver que tipo de imagenes se tiene\n",
    "#       Revisar una imagen por clase\n",
    "#       La distribucion de las clases \n",
    "#       Cualquier otro analisis que desee realizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237fc51",
   "metadata": {},
   "source": [
    "## 2. Loading Dataset and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bbfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './vegetable'\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "train_transforms = \n",
    "\n",
    "\n",
    "test_transforms = \n",
    "\n",
    "\n",
    "train_data = \n",
    "test_data = \n",
    "\n",
    "trainloader = \n",
    "testloader = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09308554",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77092a56",
   "metadata": {},
   "source": [
    "1) ¿Qué información tenemos para cada muestra? \n",
    "\n",
    "    __answer__\n",
    "\n",
    "    \n",
    "2) ¿Cuántas muestras de entrenamiento tenemos? ¿Muestras de validación? ¿Muestras de prueba? \n",
    "\n",
    "    __answer__\n",
    "\n",
    "\n",
    "3) ¿Cuántas clases diferentes hay en este conjunto de datos y cuáles son las etiquetas de clase? \n",
    "\n",
    "    __answer__\n",
    "\n",
    "\n",
    "4) Mirando las primeras 10 imágenes de entrenamiento, ¿nota algo interesante sobre las imágenes en el conjunto de datos? ¿Son tan diversos/representativos como cabría esperar o tienen limitaciones? \n",
    "\n",
    "    __answer__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef446e00",
   "metadata": {},
   "source": [
    "## 3. Simple Neural Network (MLP)\n",
    "- Crear un modelo MLP\n",
    "- Describir su arquitectura del modelo (Revisar ejemplo)\n",
    "- Entrenar el modelo, debe probar con varios hyperparameter (learning rate, batch_size, dropout)\n",
    "- Evaluar el modelo \n",
    "    - Train Accuracy vs Epochs, Val Accuracy vs Epochs\n",
    "    - Train Loss vs Epochs, Val Loss vs Epochs\n",
    "    - Confusion Matrix\n",
    "- Guardar su modelo en una carpeta `saved_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00703763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efa48a7c",
   "metadata": {},
   "source": [
    "## 4.  Simple CNN\n",
    "- Crear un modelo CNN simple (no muy profundo)\n",
    "- Describir su arquitectura del modelo (Revisar ejemplo)\n",
    "- Entrenar el modelo, debe probar con varios hyperparameter (learning rate, batch_size, dropout)\n",
    "- Evaluar el modelo \n",
    "    - Train Accuracy vs Epochs, Val Accuracy vs Epochs\n",
    "    - Train Loss vs Epochs, Val Loss vs Epochs\n",
    "    - Confusion Matrix\n",
    "- Guardar su modelo en una carpeta `saved_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1e7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dcb0506",
   "metadata": {},
   "source": [
    "## 5. Deeper CNN Model (add more CNN and max pooling) \n",
    "- Crear un modelo Deep CNN\n",
    "- Describir su arquitectura del modelo (Revisar ejemplo)\n",
    "- Entrenar el modelo, debe probar con varios hyperparameter (learning rate, batch_size, dropout)\n",
    "- Evaluar el modelo \n",
    "    - Train Accuracy vs Epochs, Val Accuracy vs Epochs\n",
    "    - Train Loss vs Epochs, Val Loss vs Epochs\n",
    "    - Confusion Matrix\n",
    "- Guardar su modelo en una carpeta `saved_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10a0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8407f83",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning\n",
    "- Crear un modelo utilizando la tecnica Transfer Learning\n",
    "- Describir su arquitectura del modelo (Revisar ejemplo)\n",
    "- Entrenar el modelo, debe probar con varios hyperparameter (learning rate, batch_size, dropout)\n",
    "- Evaluar el modelo \n",
    "    - Train Accuracy vs Epochs, Val Accuracy vs Epochs\n",
    "    - Train Loss vs Epochs, Val Loss vs Epochs\n",
    "    - Confusion Matrix\n",
    "- Guardar su modelo en una carpeta `saved_models`\n",
    "\n",
    "Puede usar alguna de las siguientes opciones:\n",
    "- VGG-16\n",
    "- ResNet\n",
    "- DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56836b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ae609b",
   "metadata": {},
   "source": [
    "## 7. Checking Predictions with the best models \n",
    "\n",
    "- Seleccione sus dos mejores modelos y comparelos.\n",
    "- Encuentre en que imagenes estos modelos se equivocan con frecuencia\n",
    "- Evalue estos modelos usando imagenes tomadas por usted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b04d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2b189e5",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "Escriba sus `findings`. Es decir, un resumen de cada modelo su accuracy, loss, metodos, dificultades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57936f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41c0d595",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595cb99",
   "metadata": {},
   "source": [
    "### 1) Explique la relación entre el batchsize y el learning rate. ¿Qué es el overfitting? ¿Cómo se puede detectar y evitar el overfitting?\n",
    "\n",
    "    __answer__\n",
    "\n",
    "### 2) Comente la diferencia de desempeño entre los modeles y la clasificación obtenida\n",
    "\n",
    "    __answer__\n",
    "    \n",
    "### 3) En que imagenes sus modelos suelen equivocarse? Por que cree que esto ocurre? Como podria mejorar lo?\n",
    "\n",
    "    __answer__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd37ed9",
   "metadata": {},
   "source": [
    "## Deployment and test on \"real\" data (Optional)\n",
    "\n",
    "La llegada del [Gradio Tutorial](https://gradio.app/quickstart/) ha facilitado mucho este proceso. Si bien aún necesitará TensorFlow o PyTorch para entrenar el modelo en sí, simplemente puede usar gradio para implementar el modelo, almacenar datos y crear una interfaz de usuario. Lo mejor de todo es que cada paso de este proceso se puede lograr con Python. ¡Puede usar esta biblioteca para crear fácilmente demos de ML!\n",
    "\n",
    "Ejemplo de `gradio`. Cuando se introduce el nombre `Jose Jesus` en la primera entrada, la segunda muestra una salida `Hello Jose Jesus!`\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    " return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "![Text](./images/image1.png)\n",
    "\n",
    "### Un ejemplo de imagen\n",
    "Gradio admite muchos tipos de componentes, como imagen, marco de datos, video o etiqueta. ¡Probemos una función de imagen a imagen para tener una idea de esto!\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189], \n",
    "        [0.349, 0.686, 0.168], \n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "![Text](./images/image2.png)\n",
    "\n",
    "## Upload your demo\n",
    "\n",
    "* **Crea una cuenta de Hugging Face y sube tu demo a Spaces**\n",
    "\n",
    "1. Cree una cuenta gratuita de Hugging Face si aún no tiene una: https://huggingface.co/login\n",
    "1. Cree un nuevo espacio **público** con el código para su aplicación Gradio. Puede encontrar útil este tutorial: https://huggingface.co/blog/gradio-spaces (Tenga en cuenta que además de cargar el código para su demostración de Gradio, también deberá cargar los archivos de modelo guardados y algunas imágenes de ejemplo, así como un archivo `requirements.txt`).\n",
    "1. Una vez que se inicie su aplicación, coloque el enlace a su espacio aquí:\n",
    "\n",
    "__answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2103b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
